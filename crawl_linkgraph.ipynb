{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c6553-d8f7-4d78-83c3-041ee1748d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814cb8ee-e896-4ede-808d-1b19d58799a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from asyncio import gather\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Note: This crawler will not run out of control because it locks stubbornly onto\n",
    "# completing each click-depth and is only recording the link graph and no on-page\n",
    "# content. This is useful for getting the URLs and visualizing the link graph.\n",
    "\n",
    "homepage = \"https://mikelev.in/blog/\"\n",
    "data = \"crawl\"\n",
    "\n",
    "Path(data).mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "max_crawl_per_run = 500\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"\n",
    "headers = {\"user-agent\": user_agent}\n",
    "linkdb = f\"{data}/link.db\"\n",
    "depthdb = f\"{data}/depth.db\"\n",
    "start = time()\n",
    "\n",
    "\n",
    "# Function to get absolute links from a URL\n",
    "def onsite_links(href):\n",
    "    response = httpx.get(href, headers=headers)\n",
    "    soup = bsoup(response.text, \"html.parser\")\n",
    "    ahrefs = soup.find_all(\"a\")\n",
    "    seen = set()\n",
    "    for link in ahrefs:\n",
    "        if \"href\" in link.attrs:\n",
    "            href = link.attrs[\"href\"]\n",
    "            # Skip kooky protocols like email\n",
    "            if \":\" in href and \"//\" not in href:\n",
    "                continue\n",
    "            # Convert relative links to absolute\n",
    "            if \"://\" not in href:\n",
    "                href = urljoin(homepage, href)\n",
    "            # Convert root slash to homepage\n",
    "            if href == \"/\":\n",
    "                href = homepage\n",
    "            # Strip stuff after hash (not formal part of URL)\n",
    "            if \"#\" in href:\n",
    "                href = href[: href.index(\"#\")]\n",
    "            # Remove dupes and offsite links\n",
    "            if href[: len(homepage)] == homepage:\n",
    "                seen.add(href)\n",
    "    return seen\n",
    "\n",
    "\n",
    "def feedback(i, t=False):\n",
    "    if not i % 1000:\n",
    "        if t:\n",
    "            print(f\"\\nProcessed: {i} of {t}\")\n",
    "        else:\n",
    "            print(f\"\\nProcessed: {i}\")\n",
    "    elif not i % 10:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "\n",
    "# Let's make some headlines!\n",
    "for i in range(1, 7):\n",
    "    func_name = f\"h{i}\"\n",
    "    num_hashes = \"#\" * i\n",
    "    command = rf\"{func_name} = lambda x: display(Markdown('{num_hashes} %s' % x))\"\n",
    "    exec(command)\n",
    "\n",
    "# Seed Crawl with click-depth 1 & 2\n",
    "h1(f\"Getting links from {homepage}\")\n",
    "links = onsite_links(homepage)\n",
    "table = []\n",
    "with sqldict(linkdb) as db:\n",
    "    db[homepage] = links\n",
    "    for link in links:\n",
    "        if link not in db:\n",
    "            db[link] = None\n",
    "            table.append(link)\n",
    "    db.commit()\n",
    "\n",
    "homepage\n",
    "\n",
    "# Record the click-depth 1 & 2 pages\n",
    "with sqldict(depthdb) as db:\n",
    "    db[homepage] = 1\n",
    "    for link in table:\n",
    "        db[link] = 2\n",
    "    db.commit()\n",
    "\n",
    "h2(\"Finding unvisited links.\")\n",
    "table = []\n",
    "with sqldict(linkdb) as db:\n",
    "    for i, url in enumerate(db):\n",
    "        feedback(i, len(db))\n",
    "        row = (url, db[url])\n",
    "        table.append(row)\n",
    "    print()\n",
    "df = pd.DataFrame(table)\n",
    "df.columns = [\"url\", \"links\"]\n",
    "df.set_index(\"url\", inplace=True)\n",
    "\n",
    "h2(\"Analyzing current click-depth.\")\n",
    "table = []\n",
    "with sqldict(depthdb) as db:\n",
    "    for i, link in enumerate(db):\n",
    "        feedback(i, len(db))\n",
    "        row = (link, db[link])\n",
    "        table.append(row)\n",
    "    print()\n",
    "df_depth = pd.DataFrame(table)\n",
    "df_depth.columns = [\"url\", \"depth\"]\n",
    "df_depth.set_index(\"url\", inplace=True)\n",
    "df = df.join([df_depth])\n",
    "max_depth = df[\"depth\"].max()\n",
    "\n",
    "to_crawl = df[(df[\"depth\"] == max_depth) & (df[\"links\"].isnull())]\n",
    "to_crawl = list(to_crawl.index)\n",
    "len_to_crawl = len(to_crawl)\n",
    "len_to_crawl\n",
    "\n",
    "if len_to_crawl:\n",
    "    if len_to_crawl < max_crawl_per_run:\n",
    "        max_crawl_per_run = len_to_crawl\n",
    "    h2(\n",
    "        f\"Visiting {max_crawl_per_run} of {len_to_crawl} pages at click-depth {max_depth}:\"\n",
    "    )\n",
    "    h3(f\"Discovering unvisited click-depth {max_depth + 1} links.\")\n",
    "    with sqldict(linkdb) as db:\n",
    "        for i, url in enumerate(to_crawl):\n",
    "            db[url] = onsite_links(url)\n",
    "            db.commit()\n",
    "            print(f\"{max_crawl_per_run - i} \", end=\"\")\n",
    "            if i >= max_crawl_per_run:\n",
    "                h4(\n",
    "                    f\"Another {max_crawl_per_run} urls will be visited each time you run.\"\n",
    "                )\n",
    "                break\n",
    "else:\n",
    "    next_depth = max_depth + 1\n",
    "    h2(f\"Done click-depth {max_depth}. Setting up tables for click-depth {next_depth}.\")\n",
    "    table = []\n",
    "    with sqldict(linkdb) as db:\n",
    "        table = []\n",
    "        for url in db:\n",
    "            links = db[url]\n",
    "            if links:\n",
    "                for link in links:\n",
    "                    table.append(link)\n",
    "    with sqldict(linkdb) as db:\n",
    "        for url in table:\n",
    "            db[url] = None\n",
    "        db.commit()\n",
    "    with sqldict(depthdb) as db:\n",
    "        for url in table:\n",
    "            if url not in db:\n",
    "                db[url] = next_depth\n",
    "        db.commit()\n",
    "    h3(f\"On the next run click-dept {next_depth} will be crawled.\")\n",
    "h3(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf3dd3-9c87-4466-a543-819abdbaf776",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "with sqldict(depthdb) as db:\n",
    "    for url in db:\n",
    "        table.append(url)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29bb0f2-c8a8-4ef1-9295-8d5cf232055d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
