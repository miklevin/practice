{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb166a-8ee4-4b0c-bb47-00b4c71023c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5fbc7-acb1-404c-ab4b-5fe0562c131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from asyncio import gather\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "\n",
    "data = \"crawl\"\n",
    "depthdb = f\"{data}/depth.db\"\n",
    "responsedb = f\"{data}/responses.db\"\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"\n",
    "headers = {\"user-agent\": user_agent}\n",
    "\n",
    "# Load URLs from link crawl\n",
    "table = []\n",
    "with sqldict(depthdb) as db:\n",
    "    for url in db:\n",
    "        table.append(url)\n",
    "print(f\"Total URLs: {len(table)}\")\n",
    "\n",
    "# Plan the crawl\n",
    "if not Path(responsedb).exists():\n",
    "    with sqldict(responsedb) as db:\n",
    "        for url in table:\n",
    "            db[url] = None\n",
    "        db.commit()\n",
    "\n",
    "# Find uncrawled URLs\n",
    "urls = []\n",
    "with sqldict(responsedb) as db:\n",
    "    uncrawled = 0\n",
    "    for url in table:\n",
    "        resposne = db[url]\n",
    "        if response == None and uncrawled <= 1000:\n",
    "            uncrawled += 1\n",
    "            urls.append(url)\n",
    "print(f\"Uncrawled URLs: {len(urls)}\")\n",
    "\n",
    "# Crawl uncrawled URLs\n",
    "async with httpx.AsyncClient(headers=headers) as client:\n",
    "    apromise = gather(*[client.get(url) for url in urls], return_exceptions=True)\n",
    "    with sqldict(responsedb, timeout=5000) as db:\n",
    "        for response in await apromise:\n",
    "            try:\n",
    "                db[str(response.url)] = response\n",
    "                db.commit()\n",
    "            except:\n",
    "                ...\n",
    "print(\"Done crawl\")\n",
    "\n",
    "c = Counter()\n",
    "with sqldict(responsedb) as db:\n",
    "    for url in db:\n",
    "        response = db[url]\n",
    "        if type(response) == httpx.Response:\n",
    "            status_code = response.status_code\n",
    "            c[status_code] += 1\n",
    "\n",
    "for status_code in c:\n",
    "    print(f\"{status_code} pages: {c[status_code]}\")\n",
    "\n",
    "table = []\n",
    "heading_tags = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "with sqldict(responsedb) as db:\n",
    "    for url in db:\n",
    "        response = db[url]\n",
    "        if type(response) == httpx.Response:\n",
    "            status_code = response.status_code\n",
    "            html = response.text\n",
    "            soup = bsoup(response.text, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                title = soup.title.string.strip()\n",
    "            except:\n",
    "                title = None\n",
    "            try:\n",
    "                description = soup.find(\"meta\", attrs={\"name\": \"description\"}).attrs[\n",
    "                    \"content\"\n",
    "                ]\n",
    "            except:\n",
    "                description = None\n",
    "            try:\n",
    "                headlines = \"\\n\\n\".join(\n",
    "                    [\n",
    "                        f\"{x.text.strip()}\"\n",
    "                        for x in soup.find_all(heading_tags)\n",
    "                        if x.text.strip()\n",
    "                    ]\n",
    "                )\n",
    "            except:\n",
    "                headlines = None\n",
    "            stripped_strings = \" \".join(soup.stripped_strings)\n",
    "            body_copy = \" \".join(\n",
    "                [x for x in [title, description, stripped_strings] if x]\n",
    "            )\n",
    "            atuple = (\n",
    "                str(response.url),\n",
    "                response.status_code,\n",
    "                title,\n",
    "                description,\n",
    "                headlines,\n",
    "                body_copy,\n",
    "                str(soup),\n",
    "            )\n",
    "            table.append(atuple)\n",
    "\n",
    "\n",
    "cols = [\"url\", \"status_code\", \"title\", \"description\", \"headlines\", \"body_copy\", \"html\"]\n",
    "\n",
    "df = pd.DataFrame(table, columns=cols)\n",
    "df.to_parquet(f\"{data}/extration.parquet\")\n",
    "print(f\"{df.shape[0]} URLs\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209a020-958a-427c-aa58-b44d001d6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c721b-fc0b-4857-997a-179df218c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa7d88-1102-4cc7-a6ec-65bb971e70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get the list of stopwords for the English language\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "owr = lambda x: x.lower()\n",
    "alfa = lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x)\n",
    "sspc = lambda x: re.sub(\" +\", \" \", x)\n",
    "flat = lambda x: \" \".join(x)\n",
    "tkns = lambda x: word_tokenize(x)\n",
    "nstp = lambda x: flat([y for y in tkns(lowr(x)) if y not in stop_words])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
