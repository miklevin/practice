{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb166a-8ee4-4b0c-bb47-00b4c71023c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5fbc7-acb1-404c-ab4b-5fe0562c131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from asyncio import gather\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from sqlitedict import SqliteDict as sqldict\n",
    "\n",
    "data = \"crawl\"\n",
    "depthdb = f\"{data}/depth.db\"\n",
    "responsedb = f\"{data}/responses.db\"\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"\n",
    "headers = {\"user-agent\": user_agent}\n",
    "\n",
    "# Load URLs from link crawl\n",
    "table = []\n",
    "with sqldict(depthdb) as db:\n",
    "    for url in db:\n",
    "        table.append(url)\n",
    "print(f\"Total URLs: {len(table)}\")\n",
    "\n",
    "# Plan the crawl\n",
    "if not Path(responsedb).exists():\n",
    "    with sqldict(responsedb) as db:\n",
    "        for url in table:\n",
    "            db[url] = None\n",
    "        db.commit()\n",
    "\n",
    "# Find uncrawled URLs\n",
    "urls = []\n",
    "with sqldict(responsedb) as db:\n",
    "    uncrawled = 0\n",
    "    for url in table:\n",
    "        response = db[url]\n",
    "        if response == None and uncrawled <= 1000:\n",
    "            uncrawled += 1\n",
    "            urls.append(url)\n",
    "print(f\"Uncrawled URLs: {len(urls)}\")\n",
    "\n",
    "# Crawl uncrawled URLs\n",
    "async with httpx.AsyncClient(headers=headers) as client:\n",
    "    apromise = gather(*[client.get(url) for url in urls], return_exceptions=True)\n",
    "    with sqldict(responsedb, timeout=5000) as db:\n",
    "        for response in await apromise:\n",
    "            try:\n",
    "                db[str(response.url)] = response\n",
    "                db.commit()\n",
    "            except:\n",
    "                ...\n",
    "print(\"Done crawl\")\n",
    "\n",
    "c = Counter()\n",
    "with sqldict(responsedb) as db:\n",
    "    for url in db:\n",
    "        response = db[url]\n",
    "        if type(response) == httpx.Response:\n",
    "            status_code = response.status_code\n",
    "            c[status_code] += 1\n",
    "\n",
    "for status_code in c:\n",
    "    print(f\"{status_code} pages: {c[status_code]}\")\n",
    "\n",
    "table = []\n",
    "heading_tags = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "with sqldict(responsedb) as db:\n",
    "    for url in db:\n",
    "        response = db[url]\n",
    "        if type(response) == httpx.Response:\n",
    "            status_code = response.status_code\n",
    "            html = response.text\n",
    "            soup = bsoup(response.text, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                title = soup.title.string.strip()\n",
    "            except:\n",
    "                title = None\n",
    "            try:\n",
    "                description = soup.find(\"meta\", attrs={\"name\": \"description\"}).attrs[\n",
    "                    \"content\"\n",
    "                ]\n",
    "            except:\n",
    "                description = None\n",
    "            try:\n",
    "                headlines = \"\\n\\n\".join(\n",
    "                    [\n",
    "                        f\"{x.text.strip()}\"\n",
    "                        for x in soup.find_all(heading_tags)\n",
    "                        if x.text.strip()\n",
    "                    ]\n",
    "                )\n",
    "            except:\n",
    "                headlines = None\n",
    "            stripped_strings = \" \".join(soup.stripped_strings)\n",
    "            body_copy = \" \".join(\n",
    "                [x for x in [title, description, stripped_strings] if x]\n",
    "            )\n",
    "            atuple = (\n",
    "                str(response.url),\n",
    "                response.status_code,\n",
    "                title,\n",
    "                description,\n",
    "                headlines,\n",
    "                body_copy,\n",
    "                str(soup),\n",
    "            )\n",
    "            table.append(atuple)\n",
    "\n",
    "\n",
    "cols = [\"url\", \"status_code\", \"title\", \"description\", \"headlines\", \"body_copy\", \"html\"]\n",
    "\n",
    "df = pd.DataFrame(table, columns=cols)\n",
    "df.to_parquet(f\"{data}/extration.parquet\", index=False)\n",
    "print(f\"{df.shape[0]} URLs\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209a020-958a-427c-aa58-b44d001d6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c721b-fc0b-4857-997a-179df218c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a67605-2286-4c05-a9dc-73c877882697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from yake import KeywordExtractor\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "site_filter = [\"mike\", \"levin\", \"seo\", \"linux\", \"python\", \"vim\", \"git\"]\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "kw_extractor = KeywordExtractor()\n",
    "\n",
    "stem = lambda x: pttm(sspc(lowr(alfa(x))))\n",
    "pttm = lambda x: porter_stemmer.stem(x)\n",
    "sspc = lambda x: re.sub(\" +\", \" \", x)\n",
    "lowr = lambda x: x.lower()\n",
    "alfa = lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x)\n",
    "brand = lambda x: flat([y for y in tkns(lowr(x)) if y not in site_filter])\n",
    "flat = lambda x: \" \".join(x)\n",
    "tkns = lambda x: word_tokenize(x)\n",
    "xkws = lambda x: kw_extractor.extract_keywords(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2dd99-df8a-4c59-bc90-7705f1cf5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aseries = df.url, df.title.apply(stem) + df.description.apply(\n",
    "    stem\n",
    ") + df.body_copy.apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c2a36-40ed-4298-8ce2-32537bf52a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = pd.DataFrame(aseries, [\"url\", \"body\"]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197b2ed-156d-47c3-8527-86641df9329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhist = dfh.url, dfh.body.apply(brand).apply(xkws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5ee44-10a4-43e6-95ad-7467293e2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(dfhist[0], dfhist[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd8c28-e189-4d68-9b2b-e6174ee77258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
